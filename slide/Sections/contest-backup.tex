% !TeX spellcheck = en_GB

\begin{frame}[fragile]{AdaBoost algorithm (Discrete AdaBoost)}

% gradient boosting
Discrete AdaBoost

{%
\setlength{\interspacetitleruled}{0pt}%
\setlength{\algotitleheightrule}{0pt}%
\begin{algorithm}[H]
\KwIn{$M$, $\set{(x_i,y_i)}_1^N$, $x_i\in\R^p$}
Initialize observation weights $w_i^{(1)}=1/N$ s.t. $\sum_{i=1}^Nw_i^{(m)}=1$\;
\For{$m=1$ \KwTo $M$}{
	Fit classifier $G_m(x)$ using $w_i^{(m)}$\;
	Weighted error rate $\text{err}_m=\sum_{i=1}^Nw_i^{(m)}\mathbb{I}(y_i\neq G(x_i))$\;
	Classifier's weight $\alpha_m=\log\bigl(\frac{1-\text{err}_m}{\text{err}_m}\bigr)$\;
	Observation weights $w_i^{(m+1)}\gets w_i^{(m)}\exp(\alpha_m\mathbb{I}(y_i\neq G(x_i)))$\;
}
\KwOut{$G(x)=\sign(\sum_{m=1}^M\alpha_mG_m(x))$}
\end{algorithm}}

%Where the \alert{weak learner} $G_m\in\set{-1,1}$ is a CART
Where the \alert{weak learner} $G_m\colon\R^p\to\set{-1,1}$ is a CART

% questa è la base di partenza
% poi è stato collegato a concetti statistici quali: loss function, modello additivo e regressione logistica
% è stato mostrato che discrete adaboost fitta una forward stagewise additive logistic regression che minimizza la exponential loss

\end{frame}

% ------------------------------- %

%\begin{frame}{Forward stagewise additive modeling}
%
%The general framework for boosting  % additive expansion of basis functions
%
%{%
%\setlength{\interspacetitleruled}{0pt}%
%\setlength{\algotitleheightrule}{0pt}%
%\begin{algorithm}[H]
%\KwIn{$M$, $\set{(x_i,y_i)}_1^N$}
%Start with $f_0(\boldsymbol{x})=0$\;
%\For{$m=1$ \KwTo $M$}{
%%	Solve
%	$(\beta_m,\gamma_m)=\argmin_{\beta,\gamma}\sum_{i=1}^NL(y_i,f_{m-1}(x_i)+\beta b(x_i;\gamma))$\;
%%	Update
%	$f_m(\boldsymbol{x})=f_{m-1}(\boldsymbol{x})+\beta_mb(\boldsymbol{x};\gamma_m)$\;
%}
%\end{algorithm}}
%
%Where $b(\boldsymbol{x};\gamma_m)\in\R$ is a basis function depending on parameter $\gamma_m$ %  the number of basis functions is chosen according to hyper-parameter M
%\end{frame}

% ------------------------------- %

\begin{frame}[fragile]{AdaBoost stochastic setting}
% stochastic setting
% stochastic gradient boosting framework, when bag.frac < 1
% -> shrinkage added
% -> out-of-bag fraction, on each iteration train a classifier on a different dataset sample, keep the rest as OOB

{%
\setlength{\interspacetitleruled}{0pt}%
\setlength{\algotitleheightrule}{0pt}%
\begin{algorithm}[H]
\KwIn{$M$, $\set{(x_i,y_i)}_1^N$, $x_i\in\R^p$}
%Initialize observation weights $w_i^{(1)}=1/N$ s.t. $\sum_{i=1}^Nw_i^{(m)}=1$\;
Initialize $f_0(x)=0$\;
\For{$m=1$ \KwTo $M$}{
	Set $w_i^{(m)}=-\frac{\partial L(y,g)}{\partial g}\bigr\rvert_{g=f_m(x)}$ s.t.  $\sum_{i=1}^Nw_i^{(m)}=1$\;
	Fit classifier $G_m(x)$ using $w_i^{(m)}$ with samples from $\pi_m$\;
	Weighted error rate $\text{err}_m=\sum_{i=1}^Nw_i^{(m)}\mathbb{I}(y_i\neq G(x_i))$\;
	Set $\alpha_m=\frac{1}{2}\log\bigl(\frac{1-\text{err}_m}{\text{err}_m}\bigr)$\;
	Update $f_{m}(x)\gets f_{m-1}(x)+\lambda\alpha_mG_m(x)$\;
}
\KwOut{$G(x)=\sign(f_M(x))$}
\end{algorithm}}

\end{frame}
