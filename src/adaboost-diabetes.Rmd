---
title: "Diabetes"
output: html_document
date: "2024-05-16"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

source("utils.R")
colors.biasvar <- c("#34558b", "#800080")

library(RColorBrewer)
colors.add <- colorRampPalette(brewer.pal(8,"Dark2"))(2)
```

## Diabetes dataset

```{r}
data <- read.csv("./data/diabetes.csv")
data$Outcome <- 2 * data$Outcome - 1
target <- which(colnames(data) == "Outcome")

dataset.distrib(data$Outcome)

summary(data)
```

Dataset di 768 donne native americane tra i 21 e gli 81 anni
- Numero di gravidanze
- Concentrazione di glucosio
- Pressione diastolica in mmHg
- Spessore della pelli ai tricipiti
- Siero di insulina valutato su 2 ore
- Body mass index
- Ereditarietà
- Età

Variables mean from the summary

```{r}
boxplot(scale(data[,-target]))
```

```{r}
# preprocess.mean <- c(mean(data$Glucose), mean(data$BloodPressure), mean(data$SkinThickness),
#                      mean(data$Insulin), mean(data$BMI))
preprocess.mean <- sapply(1:8, function(i) {mean(data[,i])})
names(preprocess.mean) <- names(data[,-target])

handle.idx <- c(2, 3, 4, 5, 6)
# names(handle.idx) <- c("Glucose", "BloodPressure", "SkinThickness", "Insulin", "BMI")
```

Substitute values with 0 with the mean from the preprocessing

```{r}
for (i in handle.idx) {
  data[,i][data[,i] == 0] <- preprocess.mean[i]
}

# data$Insulin[which(scale(data$Insulin) > 3.29 | scale(data$Insulin) < -3.29)] <- preprocess.mean["Insulin"]
```

Preprocessed summary

```{r}
summary(data)
```

Heat map

```{r}
heatmap(cor(data[,-target]), Colv=NA, Rowv=NA)
```

```{r}
n <- nrow(data)
test.n <- floor(n / 3)

set.seed(111)
train.idx <- sample(n, n - test.n, replace=FALSE)

train <- data[train.idx,]
test <- data[-train.idx,]
```

```{r}
# hist(diab.data$Insulin, prob=TRUE)
# lines(density(diab.data$Insulin), lwd=2)
```


## Decision tree

```{r}
library(rpart)
library(rpart.plot)
library(partykit)
```

### Stump

```{r}
stump <- rpart::rpart(Outcome ~ ., data=train, method="class",
                      control=rpart.control(maxdepth=1, cp=-1, minsplit=0, xval=0))
```

```{r}
print(stump)
rpart.plot::rpart.plot(stump)
plot(as.party(stump), cex=0.4)

first.metrics(train$Outcome, predict(stump, type="class"),
              test$Outcome, predict(stump, newdata=test, type="class"), FALSE)
```

### Model selection

```{r}
## fully-grown tree
tree <- rpart::rpart(Outcome ~ ., data=train, method="class",
                     control=rpart.control(minsplit=10, cp=0))

tree.train.err <- sum(1 - (predict(tree, type="class") == train$Outcome)) / nrow(train)
tree.test.err <- sum(1 - (predict(tree, type="class", newdata=test) == test$Outcome)) / nrow(test)

mincp <- tree$cptable[which.min(tree$cptable[,"xerror"]), "CP"]

## pruned tree
tree.pr <- rpart::prune(tree, cp=mincp)

tree.pr.train.err <- sum(1 - (predict(tree.pr, type="class") == train$Outcome)) / nrow(train)
tree.pr.test.err <- sum(1 - (predict(tree.pr, type="class", newdata=test) == test$Outcome)) / nrow(test)
```

```{r}
## grown tree
first.metrics(train$Outcome, predict(tree, type="class"),
              test$Outcome, predict(tree, type="class", newdata=test), FALSE)

## number of terminal nodes
paste0("Terminal nodes: ", sum(tree$frame$var == "<leaf>"))
```

```{r}
## pruned tree
rpart.plot::rpart.plot(tree.pr)
plot(as.party(tree.pr))

## metrics
first.metrics(train$Outcome, predict(tree.pr, type="class"),
              test$Outcome, predict(tree.pr, type="class", newdata=test), FALSE)

## number of terminal nodes
paste0("Terminal nodes: ", sum(tree.pr$frame$var == "<leaf>"))
```


## Boosting

```{r}
library(ada)
```

Siccome ci sono pochi esempi, si mette lo shrinkage molto basso così evita di andare immediatamente in overfitting

```{r}
nu.diab <- 0.01
pi.diab <- 0.7
```

### Model selection

```{r}
M.max <- 1000

adaboost <- ada::ada(x=as.matrix(train[,-target]), y=train$Outcome,
                     test.x=as.matrix(test[,-target]), test.y=test$Outcome,
                     loss="exponential", type="discrete",
                     iter=M.max, nu=nu.diab, bag.frac=1)

adaboost2 <- ada::ada(x=as.matrix(train[,-target]), y=train$Outcome,
                     test.x=as.matrix(test[,-target]), test.y=test$Outcome,
                     loss="exponential", type="discrete",
                     iter=M.max, nu=0.1, bag.frac=1)

adaboost3 <- ada::ada(x=as.matrix(train[,-target]), y=train$Outcome,
                     test.x=as.matrix(test[,-target]), test.y=test$Outcome,
                     loss="exponential", type="discrete",
                     iter=M.max, nu=1, bag.frac=1)

adaboost.err <- adaboost.cv(M.max, train, 5, "Outcome", lam=nu.diab)

adaboost.err2 <- adaboost.cv(M.max, train, 5, "Outcome", lam=0.1)

adaboost.err3 <- adaboost.cv(M.max, train, 5, "Outcome", lam=1)

M.cross <- seq(M.max)[which.min(adaboost.err)]
M.cross
```

```{r}
# adaboost$model$alpha
```

```{r}
plot(adaboost, kappa=FALSE, test=TRUE, cols=rainbow(dim(adaboost$model$errs)[2]+1), tflag=TRUE)

print(adaboost)

adaboost$model$errs[1:5,]
```

```{r}
## Bias-Variance tradeoff
# pdf("./plots/cv-adaboost-diab.pdf")
matplot(seq(M.max), cbind(adaboost$model$errs[,1], adaboost.err,
                          adaboost3$model$errs[,1], adaboost.err3),
        type=c("l", "l"), lty=c(1, 1), lwd=c(3,3,1.7,1.7),
        col=c(colors.biasvar, colors.add), log="x",
        xlab="Rounds of boosting", ylab="Error rate", main="Bias-Variance tradeoff",
        ylim=c(0, 0.35))

# lines(adaboost$model$errs[,3], lty=1, lwd=3, col="purple")

abline(v=M.cross, lty=2, lwd=3)

abline(h=tree.test.err, lty=3, lwd=2)
text(x=10, y=tree.test.err*0.96, cex=1.3,
     paste0(sum(tree$frame$var == "<leaf>"), "-node tree"))

# par(new=TRUE)
# plot(adaboost$model$alpha, type="l")

# abline(h=adaboost$model$errs[1,3], lty=3, lwd=2)
# text(x=12, y=adaboost$model$errs[1,3]*0.93, "Single stump", cex=1.3)

legend("bottomright",
       legend=c("Train", "CV", paste("M*=", M.cross, sep=""),
                "Train nu=1", "CV nu=1"),
       col=c(colors.biasvar, "black", colors.add),
       lty=c(1,1,2,1,1), lwd=3, cex=1.4, bg="white")
# dev.off()
```

### Final model

```{r}
adaboost.opt <- ada(x=as.matrix(train[,-target]), y=train$Outcome,
                    test.x=as.matrix(test[,-target]), test.y=test$Outcome,
                    loss="exponential", type="discrete",
                    iter=M.cross, bag.frac=1, nu=nu.diab)
```

```{r}
print(adaboost.opt)

## bias-variance tradeoff with test error
plot(adaboost.opt, FALSE, TRUE)

## variable importance
# varplot(adaboost.opt)
ada.vip <- varplot(adaboost.opt, plot.it=FALSE, type="scores")

# pdf("./plots/vip-adaboost-diab.pdf")
dotchart(rev(ada.vip), main="Variable importance", xlab="Score", pch=19, pt.cex=1.5)
# dev.off()
```

Model summary and metrics

```{r}
## summary
print(adaboost.opt)
summary(adaboost.opt)

## metrics
first.metrics(train$Outcome, predict(adaboost.opt, newdata=train, type="vector"),
              test$Outcome, predict(adaboost.opt, newdata=test, type="vector"),
              FALSE)
```


## Stochastic boosting

### Model selection

```{r}
set.seed(111)
adaboost.st <- ada::ada(x=as.matrix(train[,-target]), y=train$Outcome,
                        test.x=as.matrix(test[,-target]), test.y=test$Outcome,
                        loss="exponential", type="discrete",
                        iter=M.max, nu=nu.diab, bag.frac=pi.diab)

adaboost.st2 <- ada::ada(x=as.matrix(train[,-target]), y=train$Outcome,
                        test.x=as.matrix(test[,-target]), test.y=test$Outcome,
                        loss="exponential", type="discrete",
                        iter=M.max, nu=0.1, bag.frac=pi.diab)

adaboost.st3 <- ada::ada(x=as.matrix(train[,-target]), y=train$Outcome,
                        test.x=as.matrix(test[,-target]), test.y=test$Outcome,
                        loss="exponential", type="discrete",
                        iter=M.max, nu=1, bag.frac=pi.diab)

adaboost.st.err <- adaboost.cv(M.max, train, 5, "Outcome", nu.diab, pi.diab)

adaboost.st.err2 <- adaboost.cv(M.max, train, 5, "Outcome", 0.1, pi.diab)

adaboost.st.err3 <- adaboost.cv(M.max, train, 5, "Outcome", 1, pi.diab)

M.cross.st <- seq(M.max)[which.min(adaboost.st.err)]
M.cross.st
```

```{r}
plot(adaboost.st, kappa=FALSE, test=TRUE,
     cols=rainbow(dim(adaboost.st$model$errs)[2]+1), tflag=TRUE)

print(adaboost.st)

adaboost.st$model$errs[1:5,]
```

Il valore ottimo di iterazioni è superiore nel setting stocastico, chiaramente dovuto alla regolarizzazione di 1/10. Il paper originale comunque dice che ha maggiore influenza in una task di regressione, qua potrebbe inoltre esserci l'influenza del dataset non troppo grande.

```{r}
# pdf("./plots/cv-adaboost-st-diab.pdf")
matplot(seq(M.max), cbind(adaboost.st$model$errs[,1], adaboost.st.err,
                          adaboost.st3$model$errs[,1], adaboost.st.err3),
        type=c("l", "l"), lty=c(1, 1), lwd=c(3,3,1.7,1.7),
        col=c(colors.biasvar, colors.add), log="x",
        xlab="Rounds of boosting", ylab="Error rate", main="Bias-Variance tradeoff",
        ylim=c(0, 0.35))

# lines(adaboost.st$model$errs[,3], lty=1, lwd=3, col="purple")

abline(v=M.cross.st, lty=2, lwd=3)

abline(h=tree.test.err, lty=3, lwd=2)
text(x=20, y=tree.test.err*0.96, cex=1.3,
     paste0(sum(tree$frame$var == "<leaf>"), "-node tree"))

# abline(h=adaboost.st$model$errs[1,1], lty=3, lwd=2)
# text(x=12, y=adaboost.st$model$errs[1,1]*0.96, "Single stump", cex=0.85)

legend("bottomleft",
       legend=c("Train", "CV", paste("M*=", M.cross.st, sep=""),
                "Train nu=1", "CV nu=1"),
       col=c(colors.biasvar, "black", colors.add),
       lty=c(1,1,2,1,1), lwd=3, cex=1.4, bg="white")
# dev.off()
```

### Final model

```{r}
set.seed(111)  # only when in the stochastic setting
adaboost.st.opt <- ada(x=as.matrix(train[,-target]), y=train$Outcome,
                       test.x=as.matrix(test[,-target]), test.y=test$Outcome,
                       loss="exponential", type="discrete",
                       iter=M.cross.st, nu=nu.diab, bag.frac=pi.diab)
```

```{r}
print(adaboost.st.opt)

## bias-variance tradeoff with test error
plot(adaboost.st.opt, FALSE, TRUE)
```

Model summary and metrics

```{r}
## model summary
summary(adaboost.st.opt)

## metrics
first.metrics(train$Outcome, predict(adaboost.st.opt, newdata=train, type="vector"),
              test$Outcome, predict(adaboost.st.opt, newdata=test, type="vector"),
              FALSE)
```

### Variable importance

Si setta un seed iniziale fuori dal loop, così per ogni modello ci saranno bootstrap differenti

```{r}
vars.boost <- rep(0, dim(data)[2] - 1)
avg.run <- 30

set.seed(111)
for (i in 1:avg.run) {
  # for each m the bootstrap is different
  boost.vip.mod <- ada::ada(x=as.matrix(train[,-target]), y=train$Outcome,
                            test.x=as.matrix(test[,-target]), test.y=test$Outcome,
                            loss="exponential", type="discrete",
                            iter=M.cross.st, nu=nu.diab, bag.frac=pi.diab)
  boost.vip <- varplot(boost.vip.mod, plot.it=FALSE, type="scores")
  # set scores order by variable name
  vars.boost <- vars.boost + as.numeric(boost.vip[order(names(boost.vip))]) / avg.run
}
```

I risultati dipendono dal seed impostato, perché viene fatto un random sampling per ogni stump degli esempi sui quali verrà addestrato. Inoltre, non essendo il dataset particolarmente grande il bagging può influire negativamente.

```{r}
# changing seed, the results will be different

## variable importance for optimal adaboost
ada.st.vip <- varplot(adaboost.st.opt, plot.it=FALSE, type="scores")
# pdf("./plots/vip-adaboost-st0-diab.pdf")
dotchart(rev(ada.st.vip), main="Variable Importance", pch=19, xlab="Score", pt.cex=1.5)
# dev.off()

## variable importance for M* different adaboost
# pdf("./plots/vip-adaboost-st1-diab.pdf")
dotchart(sort(vars.boost), sort(names(boost.vip))[order(vars.boost)],
         main="Average Variable Importance", pch=19, xlab="Score", pt.cex=1.5)
# dev.off()
```
