---
title: "Diabetes"
output: html_document
date: "2024-05-16"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

source("utils.R")
colors.biasvar <- c("#34558b", "#800080")
```

## Diabetes dataset

```{r}
data <- read.csv("./data/diabetes.csv")
data$Outcome <- 2 * data$Outcome - 1
target <- which(colnames(data) == "Outcome")

dataset.distrib(data$Outcome)
```

```{r}
n <- nrow(data)
test.n <- floor(n / 3)

set.seed(111)
train.idx <- sample(n, n - test.n, replace=FALSE)

train <- data[train.idx,]
test <- data[-train.idx,]
```

```{r}
for (i in 1:8) {
  print(paste(colnames(data)[i], ": ", sd(data[,i])))
}
```

```{r}
# hist(diab.data$Insulin, prob=TRUE)
# lines(density(diab.data$Insulin), lwd=2)
```


## Decision tree

```{r}
library(rpart)
library(rpart.plot)
library(partykit)
```

### Stump

```{r}
stump <- rpart::rpart(Outcome ~ ., data=train, method="class",
                      control=rpart.control(maxdepth=1, cp=-1, minsplit=0, xval=0))
```

```{r}
print(stump)
rpart.plot::rpart.plot(stump)
plot(as.party(stump), cex=0.4)

first.metrics(train$Outcome, predict(stump, type="class"),
              test$Outcome, predict(stump, newdata=test, type="class"), FALSE)
```

### Model selection

```{r}
## fully-grown tree
tree <- rpart::rpart(Outcome ~ ., data=train, method="class",
                     control=rpart.control(minsplit=10, cp=0))

tree.train.err <- sum(1 - (predict(tree, type="class") == train$Outcome)) / nrow(train)
tree.test.err <- sum(1 - (predict(tree, type="class", newdata=test) == test$Outcome)) / nrow(test)

mincp <- tree$cptable[which.min(tree$cptable[,"xerror"]), "CP"]

## pruned tree
tree.pr <- rpart::prune(tree, cp=mincp)

tree.pr.train.err <- sum(1 - (predict(tree.pr, type="class") == train$Outcome)) / nrow(train)
tree.pr.test.err <- sum(1 - (predict(tree.pr, type="class", newdata=test) == test$Outcome)) / nrow(test)
```

```{r}
## grown tree
first.metrics(train$Outcome, predict(tree, type="class"),
              test$Outcome, predict(tree, type="class", newdata=test), FALSE)

## number of terminal nodes
paste0("Terminal nodes: ", sum(tree$frame$var == "<leaf>"))
```

```{r}
## pruned tree
rpart.plot::rpart.plot(tree.pr)
plot(as.party(tree.pr))

## metrics
first.metrics(train$Outcome, predict(tree.pr, type="class"),
              test$Outcome, predict(tree.pr, type="class", newdata=test), FALSE)

## number of terminal nodes
paste0("Terminal nodes: ", sum(tree.pr$frame$var == "<leaf>"))
```


## Boosting

```{r}
library(ada)
```

Siccome ci sono pochi esempi, si mette lo shrinkage molto basso così evita di andare immediatamente in overfitting

```{r}
nu.diab <- 0.01
pi.diab <- 0.7
```

### Model selection

```{r}
M.max <- 1000

adaboost <- ada::ada(x=as.matrix(train[,-target]), y=train$Outcome,
                     test.x=as.matrix(test[,-target]), test.y=test$Outcome,
                     loss="exponential", type="discrete",
                     iter=M.max, nu=nu.diab, bag.frac=1)

adaboost.err <- adaboost.cv(M.max, train, 5, "Outcome", lam=nu.diab)

M.cross <- seq(M.max)[which.min(adaboost.err)]
M.cross
```

```{r}
# adaboost$model$alpha
```

```{r}
plot(adaboost, kappa=FALSE, test=TRUE, cols=rainbow(dim(adaboost$model$errs)[2]+1), tflag=TRUE)

print(adaboost)

# adaboost$model$errs[1:5,]
```

```{r}
## Bias-Variance tradeoff
# pdf("./plots/cv-adaboost-diab.pdf")
matplot(seq(M.max), cbind(adaboost$model$errs[,1], adaboost.err),
        type=c("l", "l"), lty=c(1, 1), lwd=3, col=colors.biasvar, log="x",
        xlab="Rounds of boosting", ylab="Error rate", main="Bias-Variance tradeoff")

# lines(adaboost$model$errs[,3], lty=1, lwd=3, col="purple")

abline(v=M.cross, lty=2, lwd=3)

abline(h=tree.test.err, lty=3, lwd=2)
text(x=10, y=tree.test.err*0.94, cex=1.3,
     paste0(sum(tree$frame$var == "<leaf>"), "-node tree"))

# par(new=TRUE)
# plot(adaboost$model$alpha, type="l")

# abline(h=adaboost$model$errs[1,3], lty=3, lwd=2)
# text(x=12, y=adaboost$model$errs[1,3]*0.93, "Single stump", cex=1.3)

legend(150, 0.24, legend=c("Train", "CV", paste("M*=", M.cross, sep="")),
       col=c(colors.biasvar, "black"),
       lty=c(1, 1, 2), lwd=3, cex=1.5, bg="white")
# dev.off()
```

### Final model

```{r}
adaboost.opt <- ada(x=as.matrix(train[,-target]), y=train$Outcome,
                    test.x=as.matrix(test[,-target]), test.y=test$Outcome,
                    loss="exponential", type="discrete",
                    iter=M.cross, bag.frac=1, nu=nu.diab)
```

```{r}
print(adaboost.opt)

## bias-variance tradeoff with test error
plot(adaboost.opt, FALSE, TRUE)

## variable importance
# varplot(adaboost.opt)
ada.vip <- varplot(adaboost.opt, plot.it=FALSE, type="scores")

# pdf("./plots/vip-adaboost-diab.pdf")
dotchart(rev(ada.vip), main="Variable importance", xlab="Score", pch=19, pt.cex=1.5)
# dev.off()
```

```{r}
## model summary
print(adaboost.opt)
summary(adaboost.opt)

## metrics
first.metrics(train$Outcome, predict(adaboost.opt, newdata=train, type="vector"),
              test$Outcome, predict(adaboost.opt, newdata=test, type="vector"),
              FALSE)
```


## Stochastic boosting

### Model selection

```{r}
set.seed(111)
adaboost.st <- ada::ada(x=as.matrix(train[,-target]), y=train$Outcome,
                        test.x=as.matrix(test[,-target]), test.y=test$Outcome,
                        loss="exponential", type="discrete",
                        iter=M.max, nu=nu.diab, bag.frac=pi.diab)

adaboost.st.err <- adaboost.cv(M.max, train, 5, "Outcome", nu.diab, pi.diab)

M.cross.st <- seq(M.max)[which.min(adaboost.st.err)]
M.cross.st
```

```{r}
plot(adaboost.st, kappa=FALSE, test=TRUE,
     cols=rainbow(dim(adaboost.st$model$errs)[2]+1), tflag=TRUE)

print(adaboost.st)

# adaboost.st$model$errs[1:5,]
```

Il valore ottimo di iterazioni è superiore nel setting stocastico, chiaramente dovuto alla regolarizzazione di 1/10. Il paper originale comunque dice che ha maggiore influenza in una task di regressione, qua potrebbe inoltre esserci l'influenza del dataset non troppo grande.

```{r}
# pdf("./plots/cv-adaboost-st-diab.pdf")
matplot(seq(M.max), cbind(adaboost.st$model$errs[,1], adaboost.st.err),
        type=c("l", "l"), lty=c(1, 1), lwd=3, col=colors.biasvar, log="x",
        xlab="Rounds of boosting", ylab="Error rate", main="Bias-Variance tradeoff")

# lines(adaboost.st$model$errs[,3], lty=1, lwd=3, col="purple")

abline(v=M.cross.st, lty=2, lwd=3)

abline(h=tree.test.err, lty=3, lwd=2)
text(x=20, y=tree.test.err*1.04, cex=1.3,
     paste0(sum(tree$frame$var == "<leaf>"), "-node tree"))

# abline(h=adaboost.st$model$errs[1,1], lty=3, lwd=2)
# text(x=12, y=adaboost.st$model$errs[1,1]*0.96, "Single stump", cex=0.85)

legend("bottomleft", legend=c("Train", "CV", paste("M*=", M.cross.st, sep="")),
       col=c(colors.biasvar, "black"),
       lty=c(1, 1, 2), lwd=3, cex=1.5, bg="white")
# dev.off()
```

### Final model

```{r}
set.seed(111)  # only when in the stochastic setting
adaboost.st.opt <- ada(x=as.matrix(train[,-target]), y=train$Outcome,
                       test.x=as.matrix(test[,-target]), test.y=test$Outcome,
                       loss="exponential", type="discrete",
                       iter=M.cross.st, nu=nu.diab, bag.frac=pi.diab)
```

```{r}
print(adaboost.st.opt)

## bias-variance tradeoff with test error
plot(adaboost.st.opt, FALSE, TRUE)
```

```{r}
## model summary
summary(adaboost.st.opt)

## metrics
first.metrics(train$Outcome, predict(adaboost.st.opt, newdata=train, type="vector"),
              test$Outcome, predict(adaboost.st.opt, newdata=test, type="vector"),
              FALSE)
```

### Variable importance

Si setta un seed iniziale fuori dal loop, così per ogni modello ci saranno bootstrap differenti

```{r}
vars.boost <- rep(0, dim(data)[2] - 1)
avg.run <- 30

set.seed(111)
for (i in 1:avg.run) {
  # for each m the bootstrap is different
  boost.vip.mod <- ada::ada(x=as.matrix(train[,-target]), y=train$Outcome,
                            test.x=as.matrix(test[,-target]), test.y=test$Outcome,
                            loss="exponential", type="discrete",
                            iter=M.cross.st, nu=nu.diab, bag.frac=pi.diab)
  boost.vip <- varplot(boost.vip.mod, plot.it=FALSE, type="scores")
  # set scores order by variable name
  vars.boost <- vars.boost + as.numeric(boost.vip[order(names(boost.vip))]) / avg.run
}
```

I risultati dipendono dal seed impostato, perché viene fatto un random sampling per ogni stump degli esempi sui quali verrà addestrato. Inoltre, non essendo il dataset particolarmente grande il bagging può influire negativamente.

```{r}
# changing seed, the results will be different

## variable importance for optimal adaboost
ada.st.vip <- varplot(adaboost.st.opt, plot.it=FALSE, type="scores")
# pdf("./plots/vip-adaboost-st0-diab.pdf")
dotchart(rev(ada.st.vip), main="Variable Importance", pch=19, xlab="Score", pt.cex=1.5)
# dev.off()

## variable importance for M* different adaboost
# pdf("./plots/vip-adaboost-st1-diab.pdf")
dotchart(sort(vars.boost), sort(names(boost.vip))[order(vars.boost)],
         main="Average Variable Importance", pch=19, xlab="Score", pt.cex=1.5)
# dev.off()
```
