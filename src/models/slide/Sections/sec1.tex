% !TeX spellcheck = en_GB

% ------------------------------- %
%% Introduction, AdaBoost classifier %%
% ------------------------------- %

\begin{frame}{The idea behind AdaBoost \,\,i}
What are we predicting?

$\mathcal{Y}\in\set{-1,1}$
\end{frame}

% ------------------------------- %

\begin{frame}{The idea behind AdaBoost \,\,ii}
%
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\begin{figure}
\centering
\begin{tikzpicture}
	%draw,ellipse,fill=red!20,minimum height=2em,text centered,font=\sffamily\small
	\tikzset{help lines/.append style=pink}
	%\draw [help lines] (-2,-8) grid (3,1);

	\node[cloud] (m1) at (0,0) {training data};
	\node[cloud] (m2) at (0,-1.5) {weighted data};
	\node[cloud] (M) at (0,-4.5) {weighted data};
	\draw[-latex] (m1) to (m2);
	\draw[-,dotted,very thick] ($(m2.south) + (0,-0.15)$) to ($(M.north) + (0,+0.15)$);
	
	\node (G1) at (2.3,0) {$G_1(x)$};
	\node (G2) at (2.3,-1.5) {$G_2(x)$};
	\node (GM) at (2.3,-4.5) {$G_M(x)$};
	\draw[-stealth] (m1) to (G1);
	\draw[-stealth] (m2) to (G2);
%	\draw[-,dotted] (G2) to (GM);
	\draw[-stealth] (M) to (GM);
	
%	\node[rectangle,fill=pink!80] (G) at (1.5,-6) {$G(x)=\sign\Bigl(\sum_{m=1}^M\beta_mG_m(x)\Bigr)$};
	\node[rectangle,fill=mLightGreen!20] (G) at (1.5,-6.2) {$G(x)=\sign\Bigl(\sum_{m=1}^M\beta_mG_m(x)\Bigr)$};
%	\draw[-stealth] (GM) to (G);
\end{tikzpicture}
\end{figure}
\end{column}
\begin{column}{0.48\textwidth}
\vspace{0.7em}
Journey to the final classifier:
{\small\begin{itemize}
	\setlength{\itemsep}{-0.8ex}
	\item Linear combination of \alert{weak learners}
	\item Adaptively build up complexity
	\item Early stopping to achieve regularization
	\item \alert{Re-weighting} of training data  % permette all'algoritmo di concentrarsi sugli esempi pi√π difficili da classificare, quindi di questi esempi viene aumentato il peso
\end{itemize}}
\vspace{1em}
Loss function:
$L(y,f(x))=\exp(-yf(x))$
%\begin{figure}
%\centering
%\begin{tikzpicture}
%	\begin{axis}[xlabel=$yf(x)$,ylabel={$L$},axis lines=middle,enlargelimits,width=0.9\textwidth]
%		\addplot[samples=200,blue,smooth] {exp(-x)};
%		%\addplot[dashed] {1};
%%		\addplot [black, mark=-, nodes near coords=$\log(2)$, font={\scriptsize}, every node near coord/.style={anchor=180}] coordinates {(0,{ln(2)})};
%	\end{axis}
%\end{tikzpicture}
%\end{figure}
\end{column}
\end{columns}

\end{frame}

% ------------------------------- %

\begin{frame}{Forward stagewise additive modeling}

The general framework for boosting  % additive expansion of basis functions

{%
\setlength{\interspacetitleruled}{0pt}%
\setlength{\algotitleheightrule}{0pt}%
\begin{algorithm}[H]
\KwIn{$M$, $\set{(x_i,y_i)}_1^N$}
Start with $f_0(\boldsymbol{x})=0$\;
\For{$m=1$ \KwTo $M$}{
%	Solve
	$(\beta_m,\gamma_m)=\argmin_{\beta,\gamma}\sum_{i=1}^NL(y_i,f_{m-1}(x_i)+\beta b(x_i;\gamma))$\;
%	Update
	$f_m(\boldsymbol{x})=f_{m-1}(\boldsymbol{x})+\beta_mb(\boldsymbol{x};\gamma_m)$\;
}
\end{algorithm}}

Where $b(\boldsymbol{x};\gamma_m)\in\R$ is a basis function depending on parameter $\gamma_m$ %  the number of basis functions is chosen according to hyper-parameter M
\end{frame}

% ------------------------------- %

\begin{frame}[fragile]{AdaBoost algorithm}

{%
\setlength{\interspacetitleruled}{0pt}%
\setlength{\algotitleheightrule}{0pt}%
\begin{algorithm}[H]
\KwIn{$M$, $\set{(x_i,y_i)}_1^N$}
Start with $f_0(\boldsymbol{x})=0$\;
\For{$m=1$ \KwTo $M$}{
	Compute weights $w_i^{(m)}=\exp(-y_if_{m-1}(x_i))$\;
	$G_m=\argmin_G\sum_{i=1}^Nw_i^{(m)}\mathbb{I}(y_i\neq G(x_i))$\;
%	$\text{err}_m$\;
	Compute $\beta_m=\frac{1}{2}\log\bigl(\frac{1-\text{err}_m}{\text{err}_m}\bigr)$\;
	Update $f_m(\boldsymbol{x})=f_{m-1}(\boldsymbol{x})+\beta_mG_m(\boldsymbol{x})$\;
}
\KwOut{$G(\boldsymbol{x})=\sign(f_M(\boldsymbol{x}))$}
\end{algorithm}}

Where the weak learner $G_m\in\set{-1,1}$ is a CART

\end{frame}

% ------------------------------- %

%%% metrics
